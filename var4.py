# -*- coding: utf-8 -*-
"""var4

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1mBhTCxf-vMZEFCuIsFEFn9LT5zBMlW8q

Variant 4: Remove layers 9 and 10. Add two layers of (1, 1) convolution: conv (1, 1) x 128;
conv (1, 1) x10. Then add “GlobalAveragePooling2D” to merge feature maps. This is an allconvolutional
structure (no fully connected layers).
"""

# Commented out IPython magic to ensure Python compatibility.
import tensorflow as tf
from keras.datasets import cifar10
from keras.models import Sequential
from keras.layers import GlobalAveragePooling2D
from keras.layers.convolutional import Conv2D, Conv3D
from keras.layers.convolutional import MaxPooling2D, MaxPooling3D
from keras.layers.core import Activation
from keras.layers.core import Flatten
from keras.losses import CategoricalCrossentropy
from keras.utils import to_categorical
from keras.layers.core import Dropout
from keras.layers.core import Dense
from keras.layers.core import Reshape
from keras.optimizers import SGD
from sklearn.model_selection import train_test_split
from matplotlib import pyplot
import matplotlib.pyplot as plt
import numpy as np
# %matplotlib inline
import sys
import time

# using cifar-10 dataset from TensorFlow
(x_train, y_train), (x_test_base, y_test_base) = cifar10.load_data()
x_test, x_val, y_test, y_val = train_test_split(x_test_base, y_test_base, test_size=0.5)

print("x_train shape", x_train.shape)
print("x_test shape", x_test.shape)
print("x_val shape", x_val.shape)
print("y_train shape", y_train.shape)
print("y_test shape", y_test.shape)
print("y_val shape", y_val.shape)

#Prepare Dataset:
y_train = to_categorical(y_train)
y_test = to_categorical(y_test)
y_val = to_categorical(y_val)

x_train = x_train.astype("float") / 255.0 
x_test = x_test.astype("float") / 255.0 
x_val = x_val.astype("float")/255.0

class MiniVGG:
 
  def build(height, width, depth, classes):
    model = Sequential()
    InputShape = (height, width, depth)
   
    #The layer configuration:
     # first Conv3-64 -> Conv3-64 -> Maxpool(2x2) 
    #layer1
    model.add(Conv2D(64, (3, 3),  activation='relu', padding="same", input_shape=InputShape))
    #Layer2
    model.add(Conv2D(64, (3, 3), activation='relu', padding="same"))
    #Layer3
    model.add(MaxPooling2D(pool_size=(2, 2)))

    model.add(Dropout(0.25))
  

    # second Conv3-128 -> Conv3-128 -> Maxpool(2x2) 
    #Layer4
    model.add(Conv2D(128, (3, 3), activation='relu', padding="same"))
    #Layer5
    model.add(Conv2D(128, (3, 3), activation='relu', padding="same"))
    #Layer6
    model.add(MaxPooling2D(pool_size=(2, 2)))
    # third Conv3-256 -> Conv3-256 -> Maxpool(2x2)
    #Layer7  
    model.add(Conv2D(256, (3, 3), activation='relu', padding="same"))
    #Layer8
    model.add(Conv2D(256, (3, 3), activation='relu', padding="same"))
    #Remove layer 9 and 10
    #Layer9
    # model.add(MaxPooling2D(pool_size=(2, 2)))
  
    # model.add(Dropout(0.25))
   
    # # Flatten -> FC-512 -> Output
    # model.add(Flatten())
    # model.add(Dense(512,  activation='relu'))


    # New Layer 9
    model.add(Conv2D(128, (1, 1), activation='relu', padding='same'))

    # New Layer 10
    model.add(Conv2D(10, (1, 1), activation='softmax', padding='same'))
    model.add(GlobalAveragePooling2D())
   
    return model

model = MiniVGG.build(height = 32,width = 32, depth = 3, classes=10)
model.summary()

model.compile(loss=CategoricalCrossentropy(from_logits=True), optimizer=SGD(learning_rate= 0.001), metrics=['accuracy'])

#Start time before the training begins...
start_time = round(time.time() * 1000)

pred_history = model.fit(x_train, y_train, validation_data=(x_val, y_val),batch_size=64, epochs=20, verbose=1)
#Total train time...
total_train_time = round(time.time() * 1000) - start_time
print('Total Train Time: %d' % (total_train_time))

test_accuracy = model.evaluate(x_test, y_test, verbose=0)
print('Test Accuracy: %.2f' % (test_accuracy[1] * 100.0))

val_accuracy = model.evaluate(x_val, y_val, verbose=0)
print('Val Accuracy: %.2f' % (val_accuracy[1] * 100.0))

#Plot Train vs Validation Accuracy
plt.figure(figsize=(4, 6))
plt.plot(np.arange(0, 20), pred_history.history["accuracy"], label="train_acc")
plt.plot(np.arange(0, 20), pred_history.history["val_accuracy"], label="val_acc")
plt.title("Training Accuracy vs Validation Accuracy")
plt.xlabel("Number of Epochs")
plt.ylabel("Accuracy")
plt.legend()
plt.show()

#Plot Train vs Validation Loss
plt.figure(figsize=(4, 6))
plt.plot(np.arange(0, 20), pred_history.history["loss"], label="train_loss")
plt.plot(np.arange(0, 20), pred_history.history["val_loss"], label="val_loss")
plt.title("Training Loss vs Validation Loss")
plt.xlabel("Number of Epochs")
plt.ylabel("Loss")
plt.legend()
plt.show()